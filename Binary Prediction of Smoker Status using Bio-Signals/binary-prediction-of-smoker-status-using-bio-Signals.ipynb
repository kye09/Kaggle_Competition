{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60891,"databundleVersionId":6622892,"sourceType":"competition"},{"sourceId":3989074,"sourceType":"datasetVersion","datasetId":2367101},{"sourceId":149116262,"sourceType":"kernelVersion"},{"sourceId":149393795,"sourceType":"kernelVersion"},{"sourceId":150339002,"sourceType":"kernelVersion"},{"sourceId":150886438,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. INTRODUCTION","metadata":{}},{"cell_type":"markdown","source":"Dataset Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\ntrain.csv - the training dataset; smoking is the binary target\ntest.csv - the test dataset; your objective is to predict the probability of positive smoking\nsample_submission.csv - a sample submission file in the correct format","metadata":{}},{"cell_type":"markdown","source":"# 2. IMPORTS","metadata":{}},{"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport os\nimport datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom prettytable import PrettyTable\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook\ntqdm_notebook.get_lock().locks = []\n# !pip install sweetviz\n# import sweetviz as sv\nimport concurrent.futures\nfrom copy import deepcopy       \nfrom functools import partial\nfrom itertools import combinations\nimport random\nfrom random import randint, uniform\nimport gc\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom itertools import combinations\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xg\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\nfrom sklearn.cluster import KMeans\n!pip install yellowbrick\nfrom yellowbrick.cluster import KElbowVisualizer\n!pip install gap-stat\nfrom gap_statistic.optimalK import OptimalK\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import boxcox\nimport math\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.base import BaseEstimator, TransformerMixin\n!pip install optuna\nimport optuna\nimport xgboost as xgb\n!pip install catboost\n!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\nimport lightgbm as lgb\n!pip install category_encoders\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n!pip install -U imbalanced-learn\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import Pool\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.pandas.set_option('display.max_columns',None)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-05T06:47:14.50103Z","iopub.execute_input":"2023-11-05T06:47:14.501577Z","iopub.status.idle":"2023-11-05T06:48:50.174817Z","shell.execute_reply.started":"2023-11-05T06:47:14.501538Z","shell.execute_reply":"2023-11-05T06:48:50.172859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Data","metadata":{}},{"cell_type":"code","source":"global device\ndevice = 'cpu'\n\n\ntrain=pd.read_csv('/kaggle/input/playground-series-s3e24/train.csv')\ntest=pd.read_csv('/kaggle/input/playground-series-s3e24/test.csv')\noriginal=pd.read_csv(\"/kaggle/input/smoker-status-prediction-using-biosignals/train_dataset.csv\")\n\ntrain.drop(columns=[\"id\"],inplace=True)\ntest.drop(columns=[\"id\"],inplace=True)\n\ntrain_copy=train.copy()\ntest_copy=test.copy()\noriginal_copy=original.copy()\n\noriginal[\"original\"]=1\n\ntrain[\"original\"]=0\ntest[\"original\"]=0\n\ntrain=pd.concat([train,original],axis=0)\ntrain.reset_index(inplace=True,drop=True)\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-05T06:48:50.178792Z","iopub.execute_input":"2023-11-05T06:48:50.179316Z","iopub.status.idle":"2023-11-05T06:48:51.417154Z","shell.execute_reply.started":"2023-11-05T06:48:50.179274Z","shell.execute_reply":"2023-11-05T06:48:51.415623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Missing Values","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\n\ntable.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %',\"Original Missing%\"]\nfor column in train_copy.columns:\n    data_type = str(train_copy[column].dtype)\n    non_null_count_train= np.round(100-train_copy[column].count()/train_copy.shape[0]*100,1)\n    if column!='smoking':\n        non_null_count_test = np.round(100-test_copy[column].count()/test_copy.shape[0]*100,1)\n    else:\n        non_null_count_test=\"NA\"\n    non_null_count_orig= np.round(100-original_copy[column].count()/original_copy.shape[0]*100,1)\n    table.add_row([column, data_type, non_null_count_train,non_null_count_test,non_null_count_orig])\nprint(table)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-05T06:48:51.419075Z","iopub.execute_input":"2023-11-05T06:48:51.419461Z","iopub.status.idle":"2023-11-05T06:48:51.454268Z","shell.execute_reply.started":"2023-11-05T06:48:51.41943Z","shell.execute_reply":"2023-11-05T06:48:51.45264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='3'>We don't have any missing values in any of the datasets</font>","metadata":{}},{"cell_type":"markdown","source":"# 3. EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Target Distribution","metadata":{}},{"cell_type":"code","source":"def plot_pie_chart(data, title, ax):\n    data_counts = data['smoking'].value_counts()\n    labels = data_counts.index\n    sizes = data_counts.values\n    colors = [ (0.3, 0.6, 0.6), 'crimson']  \n    explode = (0.1, 0)  \n\n    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n    ax.axis('equal') \n    ax.set_title(title)\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create three subplots in a row\n\nplot_pie_chart(train_copy, \"Train smoking status Distribution\", axes[0])\nplot_pie_chart(original, \"Original smoking status Distribution\", axes[1])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-05T06:48:51.456432Z","iopub.execute_input":"2023-11-05T06:48:51.456861Z","iopub.status.idle":"2023-11-05T06:48:52.122862Z","shell.execute_reply.started":"2023-11-05T06:48:51.456828Z","shell.execute_reply":"2023-11-05T06:48:52.121142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"3\">The smoking status distribution is similar</font>","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Numerical Feature Distributions","metadata":{}},{"cell_type":"code","source":"cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique() > 2]\nn_rows = len(cont_cols)\nfig, axs = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\nsns.set_palette(\"Set3\")\nfor i, col in enumerate(cont_cols):\n    sns.violinplot(x='smoking', y=col, data=train_copy, ax=axs[i, 0])\n    axs[i, 0].set_title(f'{col.title()} Distribution by Target (Train)', fontsize=14)\n    axs[i, 0].set_xlabel('smoking', fontsize=12)\n    axs[i, 0].set_ylabel(col.title(), fontsize=12)\n    sns.despine()\n\n    sns.violinplot(x='smoking', y=col, data=original, ax=axs[i, 1])\n    axs[i, 1].set_title(f'{col.title()} Distribution by Target (Original)', fontsize=14)\n    axs[i, 1].set_xlabel('smoking', fontsize=12)\n    axs[i, 1].set_ylabel(col.title(), fontsize=12)\n    sns.despine()\n\nfig.tight_layout()\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-05T06:48:52.127086Z","iopub.execute_input":"2023-11-05T06:48:52.127555Z","iopub.status.idle":"2023-11-05T06:49:23.537115Z","shell.execute_reply.started":"2023-11-05T06:48:52.127511Z","shell.execute_reply":"2023-11-05T06:49:23.535437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  ## 3.3 Numerical Pair Plots - Original","metadata":{}},{"cell_type":"code","source":"pair_plot_cols=[f for f in cont_cols if original[f].nunique()>50]\n\nsns.set(font_scale=1)\nplt.figure(figsize=(18, 10))\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(data=original, vars=pair_plot_cols,diag_kind='kde', \n        kind='scatter', palette='muted', \n        plot_kws={'s': 20}, hue='smoking')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-05T06:49:23.539035Z","iopub.execute_input":"2023-11-05T06:49:23.539764Z","iopub.status.idle":"2023-11-05T06:56:16.15332Z","shell.execute_reply.started":"2023-11-05T06:49:23.539705Z","shell.execute_reply":"2023-11-05T06:56:16.151494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. FEATURE ENGINEERING","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">The data processing ideas are referenced from [@paddykb](https://www.kaggle.com/code/paddykb/pg-s3e24-brute-force-and-ignorance)</font>","metadata":{}},{"cell_type":"code","source":"def create_extra_features(df):\n    best = np.where(df['hearing(left)'] < df['hearing(right)'], \n                    df['hearing(left)'],  df['hearing(right)'])\n    worst = np.where(df['hearing(left)'] < df['hearing(right)'], \n                     df['hearing(right)'],  df['hearing(left)'])\n    df['hearing(left)'] = best - 1\n    df['hearing(right)'] = worst - 1\n    \n    df['eyesight(left)'] = np.where(df['eyesight(left)'] > 9, 0, df['eyesight(left)'])\n    df['eyesight(right)'] = np.where(df['eyesight(right)'] > 9, 0, df['eyesight(right)'])\n    best = np.where(df['eyesight(left)'] < df['eyesight(right)'], \n                    df['eyesight(left)'],  df['eyesight(right)'])\n    worst = np.where(df['eyesight(left)'] < df['eyesight(right)'], \n                     df['eyesight(right)'],  df['eyesight(left)'])\n    df['eyesight(left)'] = best\n    df['eyesight(right)'] = worst\n    ##\n    df['Gtp'] = np.clip(df['Gtp'], 0, 300)\n    df['HDL'] = np.clip(df['HDL'], 0, 110)\n    df['LDL'] = np.clip(df['LDL'], 0, 200)\n    df['ALT'] = np.clip(df['ALT'], 0, 150)\n    df['AST'] = np.clip(df['AST'], 0, 100)\n    df['serum creatinine'] = np.clip(df['serum creatinine'], 0, 3)  \n    \n    return df\ntrain=create_extra_features(train)\ntest=create_extra_features(test)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:56:16.155433Z","iopub.execute_input":"2023-11-05T06:56:16.156011Z","iopub.status.idle":"2023-11-05T06:56:16.267849Z","shell.execute_reply.started":"2023-11-05T06:56:16.155964Z","shell.execute_reply":"2023-11-05T06:56:16.266261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Basic Functions","metadata":{}},{"cell_type":"code","source":"def min_max_scaler(train, test, column):\n    '''\n    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator uding overall min and max\n    '''\n    sc=MinMaxScaler()\n    \n    max_val=max(train[column].max(),test[column].max())\n    min_val=min(train[column].min(),test[column].min())\n\n    train[column]=(train[column]-min_val)/(max_val-min_val)\n    test[column]=(test[column]-min_val)/(max_val-min_val)\n    \n    return train,test  \n\ndef OHE(train_df,test_df,cols,target):\n    '''\n    Function for one hot encoding, it first combined the data so that no category is missed and\n    the category with least frequency can be dropped because of redunancy\n    '''\n    combined = pd.concat([train_df, test_df], axis=0)\n    for col in cols:\n        one_hot = pd.get_dummies(combined[col])\n        counts = combined[col].value_counts()\n        min_count_category = counts.idxmin()\n        one_hot = one_hot.drop(min_count_category, axis=1)\n        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n        combined = pd.concat([combined, one_hot], axis=\"columns\")\n        combined = combined.loc[:, ~combined.columns.duplicated()]\n    \n    # split back to train and test dataframes\n    train_ohe = combined[:len(train_df)]\n    test_ohe = combined[len(train_df):]\n    test_ohe.reset_index(inplace=True,drop=True)\n    test_ohe.drop(columns=[target],inplace=True)\n    return train_ohe, test_ohe\n\nlgb_params = {\n            'n_estimators': 100,\n            'max_depth': 6,\n            \"num_leaves\": 16,\n            'learning_rate': 0.05,\n            'subsample': 0.7,\n            'colsample_bytree': 0.8,\n            #'reg_alpha': 0.25,\n            'reg_lambda': 5e-07,\n            'objective': 'regression_l2',\n            'metric': 'mean_squared_error',\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'device': device,\n        }\ndef rmse(y1,y2):\n    ''' RMSE Evaluator'''\n    return(np.sqrt(mean_squared_error(np.array(y1),np.array(y2))))\n\ndef store_missing_rows(df, features):\n    '''Function stores where missing values are located for given set of features'''\n    missing_rows = {}\n    \n    for feature in features:\n        missing_rows[feature] = df[df[feature].isnull()]\n    \n    return missing_rows\n\ndef fill_missing_numerical(train,test,target, max_iterations=10):\n    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n    train_temp=train.copy()\n    if target in train_temp.columns:\n        train_temp=train_temp.drop(columns=target)\n        \n    \n    df=pd.concat([train_temp,test],axis=\"rows\")\n    df=df.reset_index(drop=True)\n    features=[ f for f in df.columns if df[f].isna().sum()>0]\n    if len(features)>0:\n        # Step 1: Store the instances with missing values in each feature\n        missing_rows = store_missing_rows(df, features)\n\n        # Step 2: Initially fill all missing values with \"Missing\"\n        for f in features:\n            df[f]=df[f].fillna(df[f].mean())\n\n        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n        dictionary = {feature: [] for feature in features}\n\n        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n            for feature in features:\n                # Skip features with no missing values\n                rows_miss = missing_rows[feature].index\n\n                missing_temp = df.loc[rows_miss].copy()\n                non_missing_temp = df.drop(index=rows_miss).copy()\n                y_pred_prev=missing_temp[feature]\n                missing_temp = missing_temp.drop(columns=[feature])\n\n\n                # Step 3: Use the remaining features to predict missing values using Random Forests\n                X_train = non_missing_temp.drop(columns=[feature])\n                y_train = non_missing_temp[[feature]]\n\n                model= lgb.LGBMRegressor(**lgb_params)\n                model.fit(X_train, y_train, verbose=False)\n\n                # Step 4: Predict missing values for the feature and update all N features\n                y_pred = model.predict(missing_temp)\n                df.loc[rows_miss, feature] = y_pred\n                error_minimize=rmse(y_pred,y_pred_prev)\n                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n\n#         for feature, values in dictionary.items():\n#             iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n#             plt.plot(iterations, values, label=feature)  # plot the values\n#             plt.xlabel('Iterations')\n#             plt.ylabel('RMSE')\n#             plt.title('Minimization of RMSE with iterations')\n#             plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n#         plt.show()\n        train[features] = np.array(df.iloc[:train.shape[0]][features])\n        test[features] = np.array(df.iloc[train.shape[0]:][features])\n\n    return train,test","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:56:16.270155Z","iopub.execute_input":"2023-11-05T06:56:16.270625Z","iopub.status.idle":"2023-11-05T06:56:16.329534Z","shell.execute_reply.started":"2023-11-05T06:56:16.270586Z","shell.execute_reply":"2023-11-05T06:56:16.328423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.1 Numerical Transformations","metadata":{}},{"cell_type":"code","source":"cont_cols = [f for f in train.columns if pd.api.types.is_numeric_dtype(train[f]) and train[f].nunique() >50]\ncat_cols = [f for f in train.columns if train[f].nunique()!=2and f not in ['smoking']]\n\nsc=MinMaxScaler()\n\nglobal unimportant_features\nglobal overall_best_score\nglobal overall_best_col\nunimportant_features=[]\noverall_best_score=0\noverall_best_col='none'\n\nfor col in cont_cols:\n     train, test=min_max_scaler(train, test, col)\n\ndef transformer(train, test,cont_cols, target):\n    '''\n    Algorithm applies multiples transformations on selected columns and finds the best transformation using a single variable model performance\n    '''\n    global unimportant_features\n    global overall_best_score\n    global overall_best_col\n    train_copy = train.copy()\n    test_copy = test.copy()\n    table = PrettyTable()\n    table.field_names = ['Feature', 'Initial ROC_AUC', 'Transformation', 'Tranformed ROC_AUC']\n\n    for col in cont_cols:\n        \n        for c in [\"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]:\n            if c in train_copy.columns:\n                train_copy = train_copy.drop(columns=[c])\n        \n        # Log Transformation after MinMax Scaling (keeps data between 0 and 1)\n        train_copy[\"log_\"+col] = np.log1p(train_copy[col])\n        test_copy[\"log_\"+col] = np.log1p(test_copy[col])\n        \n        # Square Root Transformation\n        train_copy[\"sqrt_\"+col] = np.sqrt(train_copy[col])\n        test_copy[\"sqrt_\"+col] = np.sqrt(test_copy[col])\n        \n        # Box-Cox transformation\n        combined_data = pd.concat([train_copy[[col]], test_copy[[col]]], axis=0)\n        epsilon = 1e-5\n        transformer = PowerTransformer(method='box-cox')\n        scaled_data = transformer.fit_transform(combined_data + epsilon)\n\n        train_copy[\"bx_cx_\" + col] = scaled_data[:train_copy.shape[0]]\n        test_copy[\"bx_cx_\" + col] = scaled_data[train_copy.shape[0]:]\n        # Yeo-Johnson transformation\n        transformer = PowerTransformer(method='yeo-johnson')\n        train_copy[\"y_J_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"y_J_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Power transformation, 0.25\n        power_transform = lambda x: np.power(x + 1 - np.min(x), 0.25)\n        transformer = FunctionTransformer(power_transform)\n        train_copy[\"pow_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"pow_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Power transformation, 2\n        power_transform = lambda x: np.power(x + 1 - np.min(x), 2)\n        transformer = FunctionTransformer(power_transform)\n        train_copy[\"pow2_\"+col] = transformer.fit_transform(train_copy[[col]])\n        test_copy[\"pow2_\"+col] = transformer.transform(test_copy[[col]])\n        \n        # Log to power transformation\n        train_copy[\"log_sqrt\"+col] = np.log1p(train_copy[\"sqrt_\"+col])\n        test_copy[\"log_sqrt\"+col] = np.log1p(test_copy[\"sqrt_\"+col])\n        \n        temp_cols = [col, \"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col,  \"pow_\"+col , \"pow2_\"+col,\"log_sqrt\"+col]\n        \n        train_copy,test_copy = fill_missing_numerical(train_copy,test_copy,\"defects\",5)\n#         train_copy[temp_cols] = train_copy[temp_cols].fillna(0)\n#         test_copy[temp_cols] = test_copy[temp_cols].fillna(0)\n        \n        pca = TruncatedSVD(n_components=1)\n        x_pca_train = pca.fit_transform(train_copy[temp_cols])\n        x_pca_test = pca.transform(test_copy[temp_cols])\n        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n        temp_cols.append(col+\"_pca_comb\")\n        \n        test_copy = test_copy.reset_index(drop=True)\n        \n        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n        \n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        \n        auc_scores = []\n        \n        for f in temp_cols:\n            X = train_copy[[f]].values\n            y = train_copy[target].values\n            \n            auc = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n#                 model =   SVC(gamma=\"auto\", probability=True, random_state=42)\n                model =   LogisticRegression() # since it is a large dataset, Logistic Regression would be a good option to save time\n                model.fit(X_train,y_train)\n                y_pred = model.predict_proba(x_val)[:,1]\n                auc.append(roc_auc_score(y_val, y_pred))\n            auc_scores.append((f, np.mean(auc)))\n            \n            if overall_best_score < np.mean(auc):\n                overall_best_score = np.mean(auc)\n                overall_best_col = f\n\n            if f == col:\n                orig_auc = np.mean(auc)\n                \n        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n        cols_to_drop = [f for f in temp_cols if f != best_col]\n        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n        \n        if cols_to_drop:\n            unimportant_features = unimportant_features+cols_to_drop\n        table.add_row([col,orig_auc,best_col ,best_auc])\n    print(table)   \n    print(\"overall best CV ROC AUC score: \",overall_best_score)\n    return train_copy, test_copy\n\ntrain, test= transformer(train, test,cont_cols, \"smoking\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:56:16.331349Z","iopub.execute_input":"2023-11-05T06:56:16.332036Z","iopub.status.idle":"2023-11-05T07:02:07.241394Z","shell.execute_reply.started":"2023-11-05T06:56:16.331994Z","shell.execute_reply":"2023-11-05T07:02:07.237777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.2 Discrete Feature-->Categorical","metadata":{}},{"cell_type":"code","source":"selected_cols=[]\nfor col in cat_cols:\n    train['cat_'+col]=train[col]\n    test['cat_'+col]=test[col]\n#     cat_list=test['cat_'+col].unique()\n#     train['cat_'+col]=train['cat_'+col].apply(lambda x: x if x in cat_list else np.nan)\n    selected_cols.append('cat_'+col)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:02:07.246694Z","iopub.execute_input":"2023-11-05T07:02:07.247148Z","iopub.status.idle":"2023-11-05T07:02:07.296104Z","shell.execute_reply.started":"2023-11-05T07:02:07.247112Z","shell.execute_reply":"2023-11-05T07:02:07.294717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n    '''\n    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied, \n    where it takes a maximum of n columns and drops the rest of them treating as rare categories\n    '''\n    train_copy=train.copy()\n    test_copy=test.copy()\n    ohe_cols=[]\n    for col in extra_cols:\n        dict1=train_copy[col].value_counts().to_dict()\n        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n        rare_keys=list([*ordered.keys()][n_limit:])\n#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n        \n        train_copy[col]=train_copy[col].replace(rare_key_map)\n        test_copy[col]=test_copy[col].replace(rare_key_map)\n    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n    train_copy=train_copy.drop(columns=drop_cols)\n    test_copy=test_copy.drop(columns=drop_cols)\n    \n    return train_copy, test_copy\n\ndef cat_encoding(train, test,cat_cols, target):\n    '''Takes in a list of features and applied different categorical encoding techniques including One-hot and return the best one using \n    a single var model and other encoders if they do not have high correlation'''\n    global overall_best_score\n    global overall_best_col\n    table = PrettyTable()\n    table.field_names = ['Feature', 'Encoded Feature', 'ROC AUC Score']\n    train_copy=train.copy()\n    test_copy=test.copy()\n    train_dum = train.copy()\n    for feature in cat_cols:\n#         cat_labels = train_copy.groupby([feature])[target].mean().sort_values().index\n#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n#         train_copy[feature + \"_target\"] = train_copy[feature].map(cat_labels2)\n#         test_copy[feature + \"_target\"] = test_copy[feature].map(cat_labels2)\n\n        dic = train_copy[feature].value_counts().to_dict()\n        train_copy[feature + \"_count\"] =train_copy[feature].map(dic)\n        test_copy[feature + \"_count\"] = test_copy[feature].map(dic)\n\n        dic2=train_copy[feature].value_counts().to_dict()\n        list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n        # list1=np.arange(len(dic2.values())) # Higher rank for low count\n        dic3=dict(zip(list(dic2.keys()),list1))\n        train_copy[feature+\"_count_label\"]=train_copy[feature].replace(dic3).astype(float)\n        test_copy[feature+\"_count_label\"]=test_copy[feature].replace(dic3).astype(float)\n\n        temp_cols = [feature + \"_count\", feature + \"_count_label\"]\n        if train_copy[feature].dtype=='O':\n            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n            train_copy=train_copy.drop(columns=[feature])\n            test_copy=test_copy.drop(columns=[feature])\n        else:\n            if train_copy[feature].nunique()<=50:\n                train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n                test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n                train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n                train_copy=train_copy.drop(columns=[feature])\n                test_copy=test_copy.drop(columns=[feature])\n#                 temp_cols.append(feature)\n            else:\n                train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=10)\n            \n\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n        auc_scores = []\n\n        for f in temp_cols:\n            X = train_copy[[f]].values\n            y = train_copy[target].astype(int).values\n\n            auc = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n                model =  HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n                model.fit(X_train, y_train)\n                y_pred = model.predict_proba(x_val)[:,1]\n                auc.append(roc_auc_score(y_val,  y_pred))\n            auc_scores.append((f, np.mean(auc)))\n            if overall_best_score < np.mean(auc):\n                overall_best_score = np.mean(auc)\n                overall_best_col = f\n        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n\n        corr = train_copy[temp_cols].corr(method='pearson')\n        corr_with_best_col = corr[best_col]\n        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n        if cols_to_drop:\n            train_copy = train_copy.drop(columns=cols_to_drop)\n            test_copy = test_copy.drop(columns=cols_to_drop)\n\n        table.add_row([feature, best_col, best_auc])\n        print(feature)\n    print(table)\n    print(\"overall best CV score: \", overall_best_score)\n    return train_copy, test_copy\n\ntrain, test= cat_encoding(train, test,selected_cols, \"smoking\")\ntrain, test = fill_missing_numerical(train, test,\"smoking\",3)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:02:07.298245Z","iopub.execute_input":"2023-11-05T07:02:07.298631Z","iopub.status.idle":"2023-11-05T07:35:24.99074Z","shell.execute_reply.started":"2023-11-05T07:02:07.298598Z","shell.execute_reply":"2023-11-05T07:35:24.989126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.3 Numerical Clustering","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\ntable.field_names = ['Clustered Feature', 'ROC AUC (CV-TRAIN)']\nfor col in cont_cols:\n    sub_set=[f for f in unimportant_features if col in f]\n    temp_train=train[sub_set]\n    temp_test=test[sub_set]\n    sc=StandardScaler()\n    temp_train=sc.fit_transform(temp_train)\n    temp_test=sc.transform(temp_test)\n    model = KMeans()\n\n    # print(ideal_clusters)\n    kmeans = KMeans(n_clusters=10)\n    kmeans.fit(np.array(temp_train))\n    labels_train = kmeans.labels_\n\n    train[col+\"_unimp_cluster_WOE\"] = labels_train\n    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n\n    \n    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n    \n    X=train[[col+\"_unimp_cluster_WOE\"]].values\n    y=train[\"smoking\"].astype(int).values\n\n    auc=[]\n    for train_idx, val_idx in kf.split(X,y):\n        X_train,y_train=X[train_idx],y[train_idx]\n        x_val,y_val=X[val_idx],y[val_idx]\n        model = HistGradientBoostingClassifier(max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n        model.fit(X_train, y_train)\n        y_pred = model.predict_proba(x_val)[:,1]\n        auc.append(roc_auc_score(y_val,y_pred))\n        \n    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(auc)])\n    if overall_best_score<np.mean(auc):\n        overall_best_score=np.mean(auc)\n        overall_best_col=col+\"_unimp_cluster_WOE\"\n\nprint(table)\nprint(\"overall best CV score: \", overall_best_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:35:24.993173Z","iopub.execute_input":"2023-11-05T07:35:24.993652Z","iopub.status.idle":"2023-11-05T07:41:51.289822Z","shell.execute_reply.started":"2023-11-05T07:35:24.993612Z","shell.execute_reply":"2023-11-05T07:41:51.288263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.4 Arithmetic New Features","metadata":{}},{"cell_type":"code","source":"def better_features(train, test, target, cols, best_score):\n    new_cols = []\n    skf = KFold(n_splits=5, shuffle=True, random_state=42)  # Stratified k-fold object\n    best_list=[]\n    for i in tqdm(range(len(cols)), desc='Generating Columns'):\n        col1 = cols[i]\n        temp_df = pd.DataFrame()  # Temporary dataframe to store the generated columns\n        temp_df_test = pd.DataFrame()  # Temporary dataframe for test data\n\n        for j in range(i+1, len(cols)):\n            col2 = cols[j]\n            # Multiply\n            temp_df[col1 + '*' + col2] = train[col1] * train[col2]\n            temp_df_test[col1 + '*' + col2] = test[col1] * test[col2]\n\n            # Divide (col1 / col2)\n            temp_df[col1 + '/' + col2] = train[col1] / (train[col2] + 1e-5)\n            temp_df_test[col1 + '/' + col2] = test[col1] / (test[col2] + 1e-5)\n\n            # Divide (col2 / col1)\n            temp_df[col2 + '/' + col1] = train[col2] / (train[col1] + 1e-5)\n            temp_df_test[col2 + '/' + col1] = test[col2] / (test[col1] + 1e-5)\n\n            # Subtract\n            temp_df[col1 + '-' + col2] = train[col1] - train[col2]\n            temp_df_test[col1 + '-' + col2] = test[col1] - test[col2]\n\n            # Add\n            temp_df[col1 + '+' + col2] = train[col1] + train[col2]\n            temp_df_test[col1 + '+' + col2] = test[col1] + test[col2]\n\n        SCORES = []\n        for column in temp_df.columns:\n            scores = []\n            for train_index, val_index in skf.split(train, train[target]):\n                X_train, X_val = temp_df[column].iloc[train_index].values.reshape(-1, 1), temp_df[column].iloc[val_index].values.reshape(-1, 1)\n                y_train, y_val = train[target].astype(int).iloc[train_index], train[target].astype(int).iloc[val_index]\n                model = LogisticRegression()#HistGradientBoostingClassifier(max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n                model.fit(X_train, y_train)\n                y_pred = model.predict_proba(X_val)[:,1]\n                score = roc_auc_score( y_val, y_pred)\n                scores.append(score)\n            mean_score = np.mean(scores)\n            SCORES.append((column, mean_score))\n\n        if SCORES:\n            best_col, best_auc = sorted(SCORES, key=lambda x: x[1],reverse=True)[0]\n            corr_with_other_cols = train.drop([target] + new_cols, axis=1).corrwith(temp_df[best_col])\n            if (corr_with_other_cols.abs().max() < 0.9 or best_auc > best_score) and corr_with_other_cols.abs().max() !=1 :\n                train[best_col] = temp_df[best_col]\n                test[best_col] = temp_df_test[best_col]\n                new_cols.append(best_col)\n                print(f\"Added column '{best_col}' with ROC AUC Score: {best_auc:.4f} & Correlation {corr_with_other_cols.abs().max():.4f}\")\n\n    return train, test, new_cols","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:41:51.292004Z","iopub.execute_input":"2023-11-05T07:41:51.292454Z","iopub.status.idle":"2023-11-05T07:41:51.316889Z","shell.execute_reply.started":"2023-11-05T07:41:51.292412Z","shell.execute_reply":"2023-11-05T07:41:51.314954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selected_features=[f for f in train.columns if train[f].nunique()>2 and f not in unimportant_features]\n# train, test,new_cols=better_features(train, test, 'smoking', selected_features, overall_best_score)\n# new_cols","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:41:51.32159Z","iopub.execute_input":"2023-11-05T07:41:51.322077Z","iopub.status.idle":"2023-11-05T15:45:25.471074Z","shell.execute_reply.started":"2023-11-05T07:41:51.322039Z","shell.execute_reply":"2023-11-05T15:45:25.46789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_cols=['height(cm)*hemoglobin',\n 'eyesight(left)*Gtp',\n 'Gtp/systolic',\n 'Gtp/Cholesterol',\n 'triglyceride+hemoglobin',\n 'HDL/Gtp',\n 'Gtp/LDL',\n 'hemoglobin+Gtp',\n 'Gtp/AST',\n 'ALT*Gtp',\n 'cat_Gtp/cat_relaxation',\n 'cat_fasting blood sugar*cat_Gtp',\n 'cat_triglyceride/cat_Gtp_count',\n 'cat_HDL*cat_Gtp_count',\n 'cat_AST*cat_Gtp',\n 'cat_Gtp*cat_height(cm)_count',\n 'cat_age_count/cat_Gtp_count',\n 'cat_Gtp_count/cat_height(cm)_count',\n 'cat_weight(kg)_count/cat_Gtp_count',\n 'cat_waist(cm)_count-cat_Gtp_count',\n 'cat_eyesight(left)_count/cat_Gtp_count',\n 'cat_Gtp_count/cat_eyesight(right)_count',\n 'cat_Gtp_count/cat_systolic_count',\n 'cat_relaxation_count_label/cat_Gtp_count',\n 'cat_Gtp_count/cat_HDL_count_label',\n 'cat_hemoglobin_count_label/cat_Gtp_count',\n 'cat_Urine protein_count/cat_Gtp_count',\n 'cat_serum creatinine_count/cat_Gtp_count',\n 'cat_ALT_count*cat_Gtp_count',\n 'hemoglobin_unimp_cluster_WOE/waist(cm)_unimp_cluster_WOE',\n 'systolic_unimp_cluster_WOE+Gtp_unimp_cluster_WOE',\n 'hemoglobin_unimp_cluster_WOE/relaxation_unimp_cluster_WOE',\n 'fasting blood sugar_unimp_cluster_WOE*hemoglobin_unimp_cluster_WOE',\n 'Cholesterol_unimp_cluster_WOE/hemoglobin_unimp_cluster_WOE',\n 'triglyceride_unimp_cluster_WOE+Gtp_unimp_cluster_WOE',\n 'HDL_unimp_cluster_WOE*hemoglobin_unimp_cluster_WOE',\n 'LDL_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n 'hemoglobin_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n 'AST_unimp_cluster_WOE-Gtp_unimp_cluster_WOE',\n 'ALT_unimp_cluster_WOE+Gtp_unimp_cluster_WOE']","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:46:20.757991Z","iopub.execute_input":"2023-11-05T15:46:20.758455Z","iopub.status.idle":"2023-11-05T15:46:20.772124Z","shell.execute_reply.started":"2023-11-05T15:46:20.758422Z","shell.execute_reply":"2023-11-05T15:46:20.77018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_arithmetic_operations(train_df, test_df, expressions_list):\n    '''\n    We pass the selected arithmetic combinations\n    '''\n    for expression in expressions_list:\n        if expression not in train_df.columns:\n            # Split the expression based on operators (+, -, *, /)\n            parts = expression.split('+') if '+' in expression else \\\n                    expression.split('-') if '-' in expression else \\\n                    expression.split('*') if '*' in expression else \\\n                    expression.split('/')\n\n            # Get the DataFrame column names involved in the operation\n            cols = [col for col in parts]\n\n            # Perform the corresponding arithmetic operation based on the operator in the expression\n            if cols[0] in train_df.columns and cols[1] in train_df.columns:\n                if '+' in expression:\n                    train_df[expression] = train_df[cols[0]] + train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] + test_df[cols[1]]\n                elif '-' in expression:\n                    train_df[expression] = train_df[cols[0]] - train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] - test_df[cols[1]]\n                elif '*' in expression:\n                    train_df[expression] = train_df[cols[0]] * train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] * test_df[cols[1]]\n                elif '/' in expression:\n                    train_df[expression] = train_df[cols[0]] / (train_df[cols[1]]+1e-5)\n                    test_df[expression] = test_df[cols[0]] /( test_df[cols[1]]+1e-5)\n    \n    return train_df, test_df\n\ntrain, test = apply_arithmetic_operations(train, test, new_cols)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:46:23.838Z","iopub.execute_input":"2023-11-05T15:46:23.839003Z","iopub.status.idle":"2023-11-05T15:46:23.857004Z","shell.execute_reply.started":"2023-11-05T15:46:23.838955Z","shell.execute_reply":"2023-11-05T15:46:23.855831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.5 Feature Elimination","metadata":{}},{"cell_type":"code","source":"first_drop=[ f for f in unimportant_features if f in train.columns]\ntrain=train.drop(columns=first_drop)\ntest=test.drop(columns=first_drop)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:46:27.356009Z","iopub.execute_input":"2023-11-05T15:46:27.357397Z","iopub.status.idle":"2023-11-05T15:46:27.620803Z","shell.execute_reply.started":"2023-11-05T15:46:27.357349Z","shell.execute_reply":"2023-11-05T15:46:27.619372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_drop_list=[]\n\ntable = PrettyTable()\ntable.field_names = ['Original', 'Final Transformation', 'ROV AUC CV']\nthreshold=0.95\n# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\nbest_cols=[]\n\nfor col in cont_cols:\n    sub_set=[f for f in train.columns if (str(col) in str(f)) and (train[f].nunique()>2)]\n#     print(sub_set)\n    if len(sub_set)>2:\n        correlated_features = []\n\n        for i, feature in enumerate(sub_set):\n            # Check correlation with all remaining features\n            for j in range(i+1, len(sub_set)):\n                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n                # If correlation is greater than threshold, add to list of highly correlated features\n                if correlation > threshold:\n                    correlated_features.append(sub_set[j])\n\n        # Remove duplicate features from the list\n        correlated_features = list(set(correlated_features))\n#         print(correlated_features)\n        if len(correlated_features)>=2:\n\n            temp_train=train[correlated_features]\n            temp_test=test[correlated_features]\n            #Scale before applying PCA\n            sc=StandardScaler()\n            temp_train=sc.fit_transform(temp_train)\n            temp_test=sc.transform(temp_test)\n\n            # Initiate PCA\n            pca=TruncatedSVD(n_components=1)\n            x_pca_train=pca.fit_transform(temp_train)\n            x_pca_test=pca.transform(temp_test)\n            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n            train=pd.concat([train,x_pca_train],axis='columns')\n            test=pd.concat([test,x_pca_test],axis='columns')\n\n            # Clustering\n            model = KMeans()\n            kmeans = KMeans(n_clusters=10)\n            kmeans.fit(np.array(temp_train))\n            labels_train = kmeans.labels_\n\n            train[col+'_final_cluster'] = labels_train\n            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n\n\n            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n\n            # See which transformation along with the original is giving you the best univariate fit with target\n            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n\n            scores=[]\n\n            for f in correlated_features:\n                X=train[[f]].values\n                y=train[\"smoking\"].astype(int).values\n\n                auc=[]\n                for train_idx, val_idx in kf.split(X,y):\n                    X_train,y_train=X[train_idx],y[train_idx]\n                    X_val,y_val=X[val_idx],y[val_idx]\n\n                    model = HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n                    model.fit(X_train,y_train)\n                    y_pred = model.predict_proba(X_val)[:,1]\n                    score = roc_auc_score( y_val, y_pred)\n                    auc.append(score)\n                if f not in best_cols:\n                    scores.append((f,np.mean(auc)))\n            best_col, best_auc=sorted(scores, key=lambda x:x[1], reverse=True)[0]\n            best_cols.append(best_col)\n\n            cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n            if cols_to_drop:\n                final_drop_list=final_drop_list+cols_to_drop\n            table.add_row([col,best_col ,best_auc])\n\nprint(table)      ","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:47:53.412542Z","iopub.execute_input":"2023-11-05T15:47:53.413284Z","iopub.status.idle":"2023-11-05T15:50:56.736683Z","shell.execute_reply.started":"2023-11-05T15:47:53.413233Z","shell.execute_reply":"2023-11-05T15:50:56.731414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. FEATURE SELECTION","metadata":{}},{"cell_type":"code","source":"final_features=[f for f in train.columns if f not in ['smoking']]\nfinal_features=[*set(final_features)]\n\nsc=StandardScaler()\n\ntrain_scaled=train.copy()\ntest_scaled=test.copy()\ntrain_scaled[final_features]=sc.fit_transform(train[final_features])\ntest_scaled[final_features]=sc.transform(test[final_features])","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:51:02.781377Z","iopub.execute_input":"2023-11-05T15:51:02.781918Z","iopub.status.idle":"2023-11-05T15:51:14.99839Z","shell.execute_reply.started":"2023-11-05T15:51:02.781872Z","shell.execute_reply":"2023-11-05T15:51:14.996857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_processor(train, test):\n    '''\n    After Scaling, some of the features may be the same and can be eliminated\n    '''\n    cols=[f for f in train.columns if \"smoking\" not in f and \"OHE\" not in f]\n    train_cop=train.copy()\n    test_cop=test.copy()\n    drop_cols=[]\n    for i, feature in enumerate(cols):\n        for j in range(i+1, len(cols)):\n            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n                if cols[j] not in drop_cols:\n                    drop_cols.append(cols[j])\n    print(drop_cols)\n    train_cop.drop(columns=drop_cols,inplace=True)\n    test_cop.drop(columns=drop_cols,inplace=True)\n    \n    return train_cop, test_cop\n\n                    \ntrain_cop, test_cop=   post_processor(train_scaled, test_scaled)        \n\n# train_cop.to_csv('train_processed.csv',index=False)\n# test_cop.to_csv('test_processed.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:51:15.001082Z","iopub.execute_input":"2023-11-05T15:51:15.001528Z","iopub.status.idle":"2023-11-05T15:54:13.94629Z","shell.execute_reply.started":"2023-11-05T15:51:15.001493Z","shell.execute_reply":"2023-11-05T15:54:13.944488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_cop.drop(columns=['smoking'])\ny_train = train['smoking'].astype(int)\n\nX_test = test_cop.copy()\n\nprint(X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:54:13.949639Z","iopub.execute_input":"2023-11-05T15:54:13.95024Z","iopub.status.idle":"2023-11-05T15:54:16.271713Z","shell.execute_reply.started":"2023-11-05T15:54:13.950186Z","shell.execute_reply":"2023-11-05T15:54:16.269922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_most_important_features(X_train, y_train, n,model_input):\n    xgb_params = {\n            'n_jobs': -1,\n            'eval_metric': 'logloss',\n            'objective': 'binary:logistic',\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': 42,\n        }\n    if device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n    lgb_params = {\n            'objective': 'binary',\n            'metric': 'logloss',\n            'boosting_type': 'gbdt',\n            'random_state': 42,\n            'device': device,\n        }\n    cb_params = {\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'AUC',\n            'loss_function': 'Logloss',\n            'random_state': 42,\n            'task_type': device.upper(),\n        }\n    if 'xgb' in model_input:\n        model = xgb.XGBClassifier(**xgb_params)\n    elif 'cat' in model_input:\n        model=CatBoostClassifier(**cb_params)\n    else:\n        model=lgb.LGBMClassifier(**lgb_params)\n        \n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    auc_scores = []\n    feature_importances_list = []\n    \n    for train_idx, val_idx in kfold.split(X_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n        model.fit(X_train_fold, y_train_fold, verbose=False)\n        \n        y_pred = model.predict_proba(X_val_fold)[:,1]\n        auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n        feature_importances = model.feature_importances_\n        feature_importances_list.append(feature_importances)\n\n    avg_auc= np.mean(auc_scores)\n    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n\n    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(avg_feature_importances)]\n    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n    top_n_features = [feature[0] for feature in sorted_features[:n]]\n\n    display_features=top_n_features[:10]\n    \n    sns.set_palette(\"Set2\")\n    plt.figure(figsize=(8, 6))\n    plt.barh(range(len(display_features)), [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features])\n    plt.yticks(range(len(display_features)), display_features, fontsize=12)\n    plt.xlabel('Average Feature Importance', fontsize=14)\n    plt.ylabel('Features', fontsize=10)\n    plt.title(f'Top {10} of {n} Feature Importances with ROC AUC score {avg_auc}', fontsize=16)\n    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n    plt.grid(axis='x', linestyle='--', alpha=0.7)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n\n    # Add data labels on the bars\n    for index, value in enumerate([avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features]):\n        plt.text(value + 0.005, index, f'{value:.3f}', fontsize=12, va='center')\n\n    plt.tight_layout()\n    plt.show()\n\n    return top_n_features","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:54:16.273556Z","iopub.execute_input":"2023-11-05T15:54:16.274084Z","iopub.status.idle":"2023-11-05T15:54:16.302557Z","shell.execute_reply.started":"2023-11-05T15:54:16.27404Z","shell.execute_reply":"2023-11-05T15:54:16.3007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'cat')\nn_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'xgb')\nn_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'lgbm')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T15:54:16.306146Z","iopub.execute_input":"2023-11-05T15:54:16.306636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_imp_features=[*set(n_imp_features_xgb+n_imp_features_lgbm)]#n_imp_features_cat\nprint(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=X_train[n_imp_features]\nX_test=X_test[n_imp_features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. MODELING","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Class Weights","metadata":{}},{"cell_type":"code","source":"classes = np.unique(y_train)  \nclass_to_index = {cls: idx for idx, cls in enumerate(classes)}\ny_train_numeric = np.array([class_to_index[cls] for cls in y_train])\n\nclass_counts = np.bincount(y_train_numeric)\n\ntotal_samples = len(y_train_numeric)\n\nclass_weights = total_samples / (len(classes) * class_counts)\n\nclass_weights_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n\nprint(\"Class counts:\", class_counts)\nprint(\"Total samples:\", total_samples)\nprint(\"Class weights:\", class_weights)\nprint(\"Class weights dictionary:\", class_weights_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Models","metadata":{}},{"cell_type":"code","source":"import tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.layers import Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd=tensorflow.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\nrms = tensorflow.keras.optimizers.RMSprop()\nnadam=tensorflow.keras.optimizers.Nadam(\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n)\nlrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann = Sequential()\nann.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=lrelu))\nann.add(Dropout(0.1))\nann.add(Dense(16,  kernel_initializer='he_uniform', activation=lrelu))\nann.add(Dropout(0.1))\nann.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\nann.add(Dropout(0.1))\n\nann.add(Dense(1,  kernel_initializer='he_uniform', activation='sigmoid'))\nann.compile(loss=\"binary_crossentropy\", optimizer=sgd,metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n        self.test_size = test_size\n        self.kfold = kfold\n        self.n_splits = n_splits\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n\nclass Classifier:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.len_models = len(self.models)\n        \n    def _define_model(self):\n        xgb_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.1,\n            'max_depth': 4,\n            'subsample': 0.8,\n            'colsample_bytree': 0.1,\n            'n_jobs': -1,\n            'eval_metric': 'logloss',\n            'objective': 'binary:logistic',\n            'tree_method': 'hist',\n            'verbosity': 0,\n            'random_state': self.random_state,\n#             'class_weight':class_weights_dict,\n        }\n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n            \n        xgb_params2=xgb_params.copy() \n        xgb_params2['subsample']= 0.3\n        xgb_params2['max_depth']=8\n        xgb_params2['learning_rate']=0.005\n        xgb_params2['colsample_bytree']=0.9\n\n        xgb_params3=xgb_params.copy() \n        xgb_params3['subsample']= 0.6\n        xgb_params3['max_depth']=6\n        xgb_params3['learning_rate']=0.02\n        xgb_params3['colsample_bytree']=0.7      \n        \n        lgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 8,\n            'learning_rate': 0.02,\n            'subsample': 0.20,\n            'colsample_bytree': 0.56,\n            'reg_alpha': 0.25,\n            'reg_lambda': 5e-08,\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n#             'class_weight':class_weights_dict,\n        }\n        lgb_params2 = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 6,\n            'learning_rate': 0.05,\n            'subsample': 0.20,\n            'colsample_bytree': 0.56,\n            'reg_alpha': 0.25,\n            'reg_lambda': 5e-08,\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state,\n        }\n        lgb_params3=lgb_params.copy()  \n        lgb_params3['subsample']=0.9\n        lgb_params3['reg_lambda']=0.3461495211744402\n        lgb_params3['reg_alpha']=0.3095626288582237\n        lgb_params3['max_depth']=8\n        lgb_params3['learning_rate']=0.007\n        lgb_params3['colsample_bytree']=0.5\n\n        lgb_params4=lgb_params2.copy()  \n        lgb_params4['subsample']=0.7\n        lgb_params4['reg_lambda']=0.1\n        lgb_params4['reg_alpha']=0.2\n        lgb_params4['max_depth']=10\n        lgb_params4['learning_rate']=0.007\n        lgb_params4['colsample_bytree']=0.5\n        cb_params = {\n            'iterations': self.n_estimators,\n            'depth': 6,\n            'learning_rate': 0.1,\n            'l2_leaf_reg': 0.7,\n            'random_strength': 0.2,\n            'max_bin': 200,\n            'od_wait': 65,\n            'one_hot_max_size': 120,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'AUC',\n            'loss_function': 'Logloss',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n        }\n        cb_sym_params = cb_params.copy()\n        cb_sym_params['grow_policy'] = 'SymmetricTree'\n        cb_loss_params = cb_params.copy()\n        cb_loss_params['grow_policy'] = 'Lossguide'\n        \n        cb_params2=  cb_params.copy()\n        cb_params2['learning_rate']=0.01\n        cb_params2['depth']=8\n        \n        cb_params3={\n            'iterations': self.n_estimators,\n            'random_strength': 0.1, \n            'one_hot_max_size': 70, \n            'max_bin': 100, \n            'learning_rate': 0.008, \n            'l2_leaf_reg': 0.3, \n            'grow_policy': 'Depthwise', \n            'depth': 10, \n            'max_bin': 200,\n            'od_wait': 65,\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'eval_metric': 'AUC',\n            'loss_function': 'Logloss',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state,\n        }\n        cb_params4=  cb_params.copy()\n        cb_params4['learning_rate']=0.01\n        cb_params4['depth']=12\n        dt_params= {'min_samples_split': 30, 'min_samples_leaf': 10, 'max_depth': 8, 'criterion': 'gini'}\n        \n        models = {\n            'xgb': xgb.XGBClassifier(**xgb_params),\n#             'xgb2': xgb.XGBClassifier(**xgb_params2),\n#             'xgb3': xgb.XGBClassifier(**xgb_params3),\n            'lgb': lgb.LGBMClassifier(**lgb_params),\n#             'lgb2': lgb.LGBMClassifier(**lgb_params2),\n#             'lgb3': lgb.LGBMClassifier(**lgb_params3),\n#             'lgb4': lgb.LGBMClassifier(**lgb_params4),\n            'cat': CatBoostClassifier(**cb_params),\n#             'cat2': CatBoostClassifier(**cb_params2),\n#             'cat3': CatBoostClassifier(**cb_params2),\n#             'cat4': CatBoostClassifier(**cb_params2),\n#             \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n#             \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n#             'hist_gbm' : HistGradientBoostingClassifier (max_iter=300, learning_rate=0.001,  max_leaf_nodes=80,\n#                                                          max_depth=6,random_state=self.random_state),#class_weight=class_weights_dict, \n#             'gbdt': GradientBoostingClassifier(max_depth=6,  n_estimators=1000,random_state=self.random_state),\n#             'lr': LogisticRegression(),\n#             'rf': RandomForestClassifier(max_depth= 9,max_features= 'auto',min_samples_split= 10,\n#                                                           min_samples_leaf= 4,  n_estimators=500,random_state=self.random_state),\n# #             'svc': SVC(gamma=\"auto\", probability=True),\n# #             'knn': KNeighborsClassifier(n_neighbors=5),\n#             'mlp': MLPClassifier(random_state=self.random_state, max_iter=1000),\n#              'etr':ExtraTreesClassifier(min_samples_split=55, min_samples_leaf= 15, max_depth=10,\n#                                        n_estimators=200,random_state=self.random_state),\n#             'dt' :DecisionTreeClassifier(**dt_params,random_state=self.random_state),\n#             'ada': AdaBoostClassifier(random_state=self.random_state),\n            'ann':ann,\n                                       \n        }\n        return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Optimize Ensemble Weights","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state, n_trials=3000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", -1, 3) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n\n        auc_score = roc_auc_score(y_true, weighted_pred)\n        log_loss_score=log_loss(y_true, weighted_pred)\n        return auc_score#/log_loss_score\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Model Fit","metadata":{}},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 5\nrandom_state = 2023\nrandom_state_list = [42] # used by split_data [71]\nn_estimators = 9999 # 9999\nearly_stopping_rounds = 300\nverbose = False\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\n\n# Initialize an array for storing test predictions\ntest_predss = np.zeros(X_test.shape[0])\nensemble_score = []\nweights = []\ntrained_models = {'xgb':[], 'lgb':[]}\n\n    \nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of Regressor models\n    classifier = Classifier(n_estimators, device, random_state)\n    models = classifier.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n            if 'lgb' == name: #categorical_feature=cat_features\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#,categorical_feature=cat_features,\n                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif 'cat' ==name:\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#cat_features=cat_features,\n                          early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            else:\n                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        elif name in 'ann':\n            model.fit(X_train_, y_train_, validation_data=(X_val, y_val),batch_size=4, epochs=5,verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_)\n        \n        if name in 'ann':\n            test_pred = np.array(model.predict(X_test))[:, 0]\n            y_val_pred = np.array(model.predict(X_val))[:, 0]\n        else:\n            test_pred = model.predict_proba(X_test)[:, 1]\n            y_val_pred = model.predict_proba(X_val)[:, 1]\n\n        score = roc_auc_score(y_val, y_val_pred)\n#         score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] ROC AUC score: {score:.5f}')\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n        \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n    # Use Optuna to find the best ensemble weights\n    optweights = OptunaWeights(random_state=random_state)\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    \n    score = roc_auc_score(y_val, y_val_pred)\n#     score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] ------------------>  ROC AUC score {score:.5f}')\n    ensemble_score.append(score)\n    weights.append(optweights.weights)\n    \n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n    \n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean ROC AUC  score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble ROC AUC score {mean_score:.5f}  {std_score:.5f}')\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f}  {std_weight:.5f}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 Feature Importance Visualization","metadata":{}},{"cell_type":"code","source":"def visualize_importance(models, feature_cols, title, head=15):\n    importances = []\n    feature_importance = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df[\"importance\"] = model.feature_importances_\n        _df[\"feature\"] = pd.Series(feature_cols)\n        _df[\"fold\"] = i\n        _df = _df.sort_values('importance', ascending=False)\n        _df = _df.head(head)\n        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n        \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n    plt.figure(figsize=(18, 10))\n    sns.barplot(x='importance', y='feature', data=feature_importance, color= (0.4, 0.76, 0.65), errorbar='sd')\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f'{title} Feature Importance', fontsize=18)\n    plt.grid(True, axis='x')\n    plt.show()\n    \nfor name, models in trained_models.items():\n    visualize_importance(models, list(X_train.columns), name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.6 Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/playground-series-s3e24/sample_submission.csv')\nsub['smoking'] =  test_predss\nsub.to_csv('submission_pure.csv',index=False)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.7 Ensemble - Averaging","metadata":{}},{"cell_type":"code","source":"sub1=pd.read_csv(\"/kaggle/input/pg-s3e24-brute-force-and-ignorance/submission.csv\")\nsub2=pd.read_csv(\"/kaggle/input/efficient-prediction-of-smoker-status/xgb_pseudo_opt_submission.csv\")\nsub3=pd.read_csv(\"/kaggle/input/pg-s3-e24-eda-modeling-ensemle-nn/nn_submission.csv\")\n\ndef scale(df):\n    df['smoking']=(df['smoking']-df['smoking'].min())/(df['smoking'].max()-df['smoking'].min())\n    return df\n\nsub_combined=sub1.copy()\n\nsub1=scale(sub1)\nsub2=scale(sub2)\nsub3=scale(sub3)\nsub=scale(sub)\n\nsub_combined['smoking']=(3*sub1['smoking'] + sub2['smoking'] +sub[\"smoking\"])/5\n\nsub_combined.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T06:46:27.801849Z","iopub.status.idle":"2023-11-05T06:46:27.802485Z","shell.execute_reply.started":"2023-11-05T06:46:27.802174Z","shell.execute_reply":"2023-11-05T06:46:27.802204Z"},"trusted":true},"execution_count":null,"outputs":[]}]}